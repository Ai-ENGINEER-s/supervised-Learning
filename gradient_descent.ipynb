{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT Descent for Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy : is popular library for scientific computaing \n",
    "# matplotlib : it's also a popular library for plotting data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math , copy \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 2.0])\n",
    "y_train = np.array([300.0 , 500.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x , y , w ,b ):\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w*x[i]+b \n",
    "        cost = cost + (f_wb - y)**2\n",
    "    cost_total_value = 1 /(2*m)*cost\n",
    "\n",
    "    return cost_total_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x , y ,w ,b ): \n",
    "    \n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db =0 \n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w*x[i]+ b \n",
    "        dj_dw_i = (f_wb - y[i])*x[i]\n",
    "        dj_db_i = (f_wb - y[i])\n",
    "    dj_dw = (dj_dw_i)/ m\n",
    "    dj_db = (dj_db_i) / m \n",
    "\n",
    "    return dj_db , dj_dw\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : Target values\n",
    "      w (scalar)        : Model parameter\n",
    "      b (scalar)        : Model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (float): The cost of using w, b as parameters for linear regression.\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    cost = (1 / (2 * m)) * np.sum((w * x + b - y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def compute_gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): Target values\n",
    "      w,b (scalar)    : Model parameters\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameter w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(x)\n",
    "    dj_dw = (1 / m) * np.sum((w * x + b - y) * x)\n",
    "    dj_db = (1 / m) * np.sum(w * x + b - y)\n",
    "    \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : Target values\n",
    "      w_in,b_in (scalar): Initial values of model parameters  \n",
    "      alpha (float)     : Learning rate\n",
    "      num_iters (int)   : Number of iterations to run gradient descent\n",
    "      cost_function     : Function to call to produce cost\n",
    "      gradient_function : Function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (List): History of parameters [w,b] \n",
    "    \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    \n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        # Update Parameters using the gradient descent rule\n",
    "        b = b - alpha * dj_db\n",
    "        w = w - alpha * dj_dw\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhau stion\n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            p_history.append([w, b])\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e}  \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "    \n",
    "    return w, b, J_history, p_history  # return w, b and cost, w history for graphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1.71e+01   dj_dw: -2.200e+01, dj_db: -6.000e+00   w:  2.200e-01, b: 6.00000e-02\n",
      "Iteration  100: Cost 1.71e-02   dj_dw: -2.042e-02, dj_db:  7.342e-02   w:  1.880e+00, b: 4.33677e-01\n",
      "Iteration  200: Cost 1.22e-02   dj_dw: -1.717e-02, dj_db:  6.201e-02   w:  1.899e+00, b: 3.66176e-01\n",
      "Iteration  300: Cost 8.70e-03   dj_dw: -1.450e-02, dj_db:  5.235e-02   w:  1.914e+00, b: 3.09179e-01\n",
      "Iteration  400: Cost 6.20e-03   dj_dw: -1.224e-02, dj_db:  4.421e-02   w:  1.928e+00, b: 2.61055e-01\n",
      "Iteration  500: Cost 4.42e-03   dj_dw: -1.034e-02, dj_db:  3.732e-02   w:  1.939e+00, b: 2.20421e-01\n",
      "Iteration  600: Cost 3.15e-03   dj_dw: -8.729e-03, dj_db:  3.152e-02   w:  1.948e+00, b: 1.86112e-01\n",
      "Iteration  700: Cost 2.25e-03   dj_dw: -7.370e-03, dj_db:  2.661e-02   w:  1.956e+00, b: 1.57143e-01\n",
      "Iteration  800: Cost 1.60e-03   dj_dw: -6.223e-03, dj_db:  2.247e-02   w:  1.963e+00, b: 1.32683e-01\n",
      "Iteration  900: Cost 1.14e-03   dj_dw: -5.255e-03, dj_db:  1.897e-02   w:  1.969e+00, b: 1.12031e-01\n",
      "Final values: w = 1.9737548787242036, b = 0.09475321533750963\n"
     ]
    }
   ],
   "source": [
    "# Initialiser les paramètres\n",
    "w_in = 0  # valeur initiale pour w\n",
    "b_in = 0  # valeur initiale pour b\n",
    "alpha = 0.01  # taux d'apprentissage\n",
    "num_iters = 1000  # nombre d'itérations\n",
    "\n",
    "# Exemple de données d'entraînement (x_train, y_train)\n",
    "x_train = np.array([1, 2, 3, 4, 5])  # Taille\n",
    "y_train = np.array([2, 4, 6, 8, 10])  # Prix\n",
    "\n",
    "# Exécuter la descente de gradient\n",
    "w, b, J_history, p_history = gradient_descent(x_train, y_train, w_in, b_in, alpha, num_iters, compute_cost, compute_gradient)\n",
    "\n",
    "print(f\"Final values: w = {w}, b = {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rebuild this thing again \n",
    "\n",
    "\n",
    "def compute_cost(x , y, w , b ):\n",
    "\n",
    "    m = x.shape[0]\n",
    "\n",
    "    f_wb = w*x + b\n",
    "    cost_function = np.sum((f_wb-y)**2)\n",
    "    total_costFunction = (1 / (2*m)*(cost_function))\n",
    "\n",
    "    return total_costFunction  \n",
    "\n",
    "\n",
    "def compute_gradient(x , y, w ,b ): \n",
    "\n",
    "    m = len(x)\n",
    "\n",
    "    dj_dw = 0 \n",
    "    dj_db = 0 \n",
    "\n",
    "    dj_dw =  np.sum(x * (w*x + b - y))\n",
    "    dj_db = np.sum((w*x +b - y))\n",
    "\n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    b = b_in\n",
    "    w= w_in\n",
    "\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        # Update Parameters using the gradient descent rule\n",
    "        b = b - alpha * dj_db\n",
    "        w = w - alpha * dj_dw\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhaustion\n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            p_history.append([w, b])\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e}  \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "    \n",
    "    return w, b, J_history, p_history  # return w, b and cost, w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1.71e+01   dj_dw: -2.200e+01, dj_db: -6.000e+00   w:  2.200e-01, b: 6.00000e-02\n",
      "Iteration  100: Cost 1.71e-02   dj_dw: -2.042e-02, dj_db:  7.342e-02   w:  1.880e+00, b: 4.33677e-01\n",
      "Iteration  200: Cost 1.22e-02   dj_dw: -1.717e-02, dj_db:  6.201e-02   w:  1.899e+00, b: 3.66176e-01\n",
      "Iteration  300: Cost 8.70e-03   dj_dw: -1.450e-02, dj_db:  5.235e-02   w:  1.914e+00, b: 3.09179e-01\n",
      "Iteration  400: Cost 6.20e-03   dj_dw: -1.224e-02, dj_db:  4.421e-02   w:  1.928e+00, b: 2.61055e-01\n",
      "Iteration  500: Cost 4.42e-03   dj_dw: -1.034e-02, dj_db:  3.732e-02   w:  1.939e+00, b: 2.20421e-01\n",
      "Iteration  600: Cost 3.15e-03   dj_dw: -8.729e-03, dj_db:  3.152e-02   w:  1.948e+00, b: 1.86112e-01\n",
      "Iteration  700: Cost 2.25e-03   dj_dw: -7.370e-03, dj_db:  2.661e-02   w:  1.956e+00, b: 1.57143e-01\n",
      "Iteration  800: Cost 1.60e-03   dj_dw: -6.223e-03, dj_db:  2.247e-02   w:  1.963e+00, b: 1.32683e-01\n",
      "Iteration  900: Cost 1.14e-03   dj_dw: -5.255e-03, dj_db:  1.897e-02   w:  1.969e+00, b: 1.12031e-01\n",
      "Final values: w = 1.9737548787242036, b = 0.09475321533750963\n"
     ]
    }
   ],
   "source": [
    "# Initialiser les paramètres\n",
    "w_in = 0  # valeur initiale pour w\n",
    "b_in = 0  # valeur initiale pour b\n",
    "alpha = 0.01  # taux d'apprentissage\n",
    "num_iters = 1000  # nombre d'itérations\n",
    "\n",
    "# Exemple de données d'entraînement (x_train, y_train)\n",
    "x_train = np.array([1, 2, 3, 4, 5])  # Taille\n",
    "y_train = np.array([2, 4, 6, 8, 10])  # Prix\n",
    "\n",
    "# Exécuter la descente de gradient\n",
    "w, b, J_history, p_history = gradient_descent(x_train, y_train, w_in, b_in, alpha, num_iters, compute_cost, compute_gradient)\n",
    "\n",
    "print(f\"Final values: w = {w}, b = {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
