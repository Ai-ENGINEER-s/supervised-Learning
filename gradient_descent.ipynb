{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT Descent for Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy : is popular library for scientific computaing \n",
    "# matplotlib : it's also a popular library for plotting data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math , copy \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 2.0])\n",
    "y_train = np.array([300.0 , 500.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x , y , w ,b ):\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w*x[i]+b \n",
    "        cost = cost + (f_wb - y)**2\n",
    "    cost_total_value = 1 /(2*m)*cost\n",
    "\n",
    "    return cost_total_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x , y ,w ,b ): \n",
    "    \n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db =0 \n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w*x[i]+ b \n",
    "        dj_dw_i = (f_wb - y[i])*x[i]\n",
    "        dj_db_i = (f_wb - y[i])\n",
    "    dj_dw = (dj_dw_i)/ m\n",
    "    dj_db = (dj_db_i) / m \n",
    "\n",
    "    return dj_db , dj_dw\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : Target values\n",
    "      w (scalar)        : Model parameter\n",
    "      b (scalar)        : Model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (float): The cost of using w, b as parameters for linear regression.\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    cost = (1 / (2 * m)) * np.sum((w * x + b - y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def compute_gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): Target values\n",
    "      w,b (scalar)    : Model parameters\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameter w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(x)\n",
    "    dj_dw = (1 / m) * np.sum((w * x + b - y) * x)\n",
    "    dj_db = (1 / m) * np.sum(w * x + b - y)\n",
    "    \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : Target values\n",
    "      w_in,b_in (scalar): Initial values of model parameters  \n",
    "      alpha (float)     : Learning rate\n",
    "      num_iters (int)   : Number of iterations to run gradient descent\n",
    "      cost_function     : Function to call to produce cost\n",
    "      gradient_function : Function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (List): History of parameters [w,b] \n",
    "    \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    \n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        # Update Parameters using the gradient descent rule\n",
    "        b = b - alpha * dj_db\n",
    "        w = w - alpha * dj_dw\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhau stion\n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            p_history.append([w, b])\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e}  \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "    \n",
    "    return w, b, J_history, p_history  # return w, b and cost, w history for graphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1.71e+01   dj_dw: -2.200e+01, dj_db: -6.000e+00   w:  2.200e-01, b: 6.00000e-02\n",
      "Iteration  100: Cost 1.71e-02   dj_dw: -2.042e-02, dj_db:  7.342e-02   w:  1.880e+00, b: 4.33677e-01\n",
      "Iteration  200: Cost 1.22e-02   dj_dw: -1.717e-02, dj_db:  6.201e-02   w:  1.899e+00, b: 3.66176e-01\n",
      "Iteration  300: Cost 8.70e-03   dj_dw: -1.450e-02, dj_db:  5.235e-02   w:  1.914e+00, b: 3.09179e-01\n",
      "Iteration  400: Cost 6.20e-03   dj_dw: -1.224e-02, dj_db:  4.421e-02   w:  1.928e+00, b: 2.61055e-01\n",
      "Iteration  500: Cost 4.42e-03   dj_dw: -1.034e-02, dj_db:  3.732e-02   w:  1.939e+00, b: 2.20421e-01\n",
      "Iteration  600: Cost 3.15e-03   dj_dw: -8.729e-03, dj_db:  3.152e-02   w:  1.948e+00, b: 1.86112e-01\n",
      "Iteration  700: Cost 2.25e-03   dj_dw: -7.370e-03, dj_db:  2.661e-02   w:  1.956e+00, b: 1.57143e-01\n",
      "Iteration  800: Cost 1.60e-03   dj_dw: -6.223e-03, dj_db:  2.247e-02   w:  1.963e+00, b: 1.32683e-01\n",
      "Iteration  900: Cost 1.14e-03   dj_dw: -5.255e-03, dj_db:  1.897e-02   w:  1.969e+00, b: 1.12031e-01\n",
      "Final values: w = 1.9737548787242036, b = 0.09475321533750963\n"
     ]
    }
   ],
   "source": [
    "# Initialiser les paramètres\n",
    "w_in = 0  # valeur initiale pour w\n",
    "b_in = 0  # valeur initiale pour b\n",
    "alpha = 0.01  # taux d'apprentissage\n",
    "num_iters = 1000  # nombre d'itérations\n",
    "\n",
    "# Exemple de données d'entraînement (x_train, y_train)\n",
    "x_train = np.array([1, 2, 3, 4, 5])  # Taille\n",
    "y_train = np.array([2, 4, 6, 8, 10])  # Prix\n",
    "\n",
    "# Exécuter la descente de gradient\n",
    "w, b, J_history, p_history = gradient_descent(x_train, y_train, w_in, b_in, alpha, num_iters, compute_cost, compute_gradient)\n",
    "\n",
    "print(f\"Final values: w = {w}, b = {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rebuild this thing again \n",
    "\n",
    "\n",
    "def compute_cost(x , y, w , b ):\n",
    "\n",
    "    m = x.shape[0]\n",
    "\n",
    "    f_wb = w*x + b\n",
    "    cost_function = np.sum((f_wb-y)**2)\n",
    "    total_costFunction = (1 / (2*m)*(cost_function))\n",
    "\n",
    "    return total_costFunction  \n",
    "\n",
    "\n",
    "def compute_gradient(x , y, w ,b ): \n",
    "\n",
    "    m = len(x)\n",
    "\n",
    "    dj_dw = 0 \n",
    "    dj_db = 0 \n",
    "\n",
    "    dj_dw =  np.sum(x * (w*x + b - y))\n",
    "    dj_db = np.sum((w*x +b - y))\n",
    "\n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    b = b_in\n",
    "    w= w_in\n",
    "\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        # Update Parameters using the gradient descent rule\n",
    "        b = b - alpha * dj_db\n",
    "        w = w - alpha * dj_dw\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhaustion\n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            p_history.append([w, b])\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e}  \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "    \n",
    "    return w, b, J_history, p_history  # return w, b and cost, w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1.71e+01   dj_dw: -2.200e+01, dj_db: -6.000e+00   w:  2.200e-01, b: 6.00000e-02\n",
      "Iteration  100: Cost 1.71e-02   dj_dw: -2.042e-02, dj_db:  7.342e-02   w:  1.880e+00, b: 4.33677e-01\n",
      "Iteration  200: Cost 1.22e-02   dj_dw: -1.717e-02, dj_db:  6.201e-02   w:  1.899e+00, b: 3.66176e-01\n",
      "Iteration  300: Cost 8.70e-03   dj_dw: -1.450e-02, dj_db:  5.235e-02   w:  1.914e+00, b: 3.09179e-01\n",
      "Iteration  400: Cost 6.20e-03   dj_dw: -1.224e-02, dj_db:  4.421e-02   w:  1.928e+00, b: 2.61055e-01\n",
      "Iteration  500: Cost 4.42e-03   dj_dw: -1.034e-02, dj_db:  3.732e-02   w:  1.939e+00, b: 2.20421e-01\n",
      "Iteration  600: Cost 3.15e-03   dj_dw: -8.729e-03, dj_db:  3.152e-02   w:  1.948e+00, b: 1.86112e-01\n",
      "Iteration  700: Cost 2.25e-03   dj_dw: -7.370e-03, dj_db:  2.661e-02   w:  1.956e+00, b: 1.57143e-01\n",
      "Iteration  800: Cost 1.60e-03   dj_dw: -6.223e-03, dj_db:  2.247e-02   w:  1.963e+00, b: 1.32683e-01\n",
      "Iteration  900: Cost 1.14e-03   dj_dw: -5.255e-03, dj_db:  1.897e-02   w:  1.969e+00, b: 1.12031e-01\n",
      "Final values: w = 1.9737548787242036, b = 0.09475321533750963\n"
     ]
    }
   ],
   "source": [
    "# Initialiser les paramètres\n",
    "w_in = 0  # valeur initiale pour w\n",
    "b_in = 0  # valeur initiale pour b\n",
    "alpha = 0.01  # taux d'apprentissage\n",
    "num_iters = 1000  # nombre d'itérations\n",
    "\n",
    "# Exemple de données d'entraînement (x_train, y_train)\n",
    "x_train = np.array([1, 2, 3, 4, 5])  # Taille\n",
    "y_train = np.array([2, 4, 6, 8, 10])  # Prix\n",
    "\n",
    "# Exécuter la descente de gradient\n",
    "w, b, J_history, p_history = gradient_descent(x_train, y_train, w_in, b_in, alpha, num_iters, compute_cost, compute_gradient)\n",
    "\n",
    "print(f\"Final values: w = {w}, b = {b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression for Multiple features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduction to numpy for multiple linear regression \n",
    "import numpy as np \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4) :   a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.zeros(4,) :  a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.random.random_sample(4): a = [0.62793365 0.35974625 0.78114418 0.03329643], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "# NumPy routines which allocate memory and fill arrays with value\n",
    "a = np.zeros(4);                print(f\"np.zeros(4) :   a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.zeros((4,));             print(f\"np.zeros(4,) :  a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.random_sample(4); print(f\"np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " values of [0. 1. 2. 3.]\n",
      "New values of variable a is [0.64314563 0.96526732 0.48313028 0.64264979]\n",
      "values of [0 1 2 3 4 5 6 7 8 9]\n",
      "the first element of a is 0\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(4.)\n",
    "print(f\" values of {a}\")\n",
    "a = np.random.rand(4)\n",
    "print(f\"New values of variable a is {a}\")\n",
    "\n",
    "# operations on vectors \n",
    "\n",
    "a = np.arange(10)\n",
    "print(f\"values of {a}\")\n",
    "\n",
    "# access an elment of a \n",
    "\n",
    "a_1 = a[0]\n",
    "print(f\"the first element of a is {a_1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_function(a , b): \n",
    "    \"compute dot product for vector with the first solution for loop \"\n",
    "\n",
    "    x = 0 \n",
    "    m = a.shape[0]\n",
    "\n",
    "    for i in range(m):\n",
    "        x = x + a[i]*b[i]\n",
    "    return x \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of our dot function : 74902\n"
     ]
    }
   ],
   "source": [
    "# test-1 \n",
    "a = np.array([41, 78 ,89])\n",
    "b = np.array([85, 45 ,763])\n",
    "\n",
    "print(f\"value of our dot function : {dot_function(a,b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 1-D np.dot(a, b) = 74902, np.dot(a, b).shape = () \n",
      "NumPy 1-D np.dot(b, a) = 74902, np.dot(a, b).shape = () \n"
     ]
    }
   ],
   "source": [
    "# Now let's try the second solution with numpy library dot function \n",
    "\n",
    "a = np.array([41, 78 ,89])\n",
    "b = np.array([85, 45 ,763])\n",
    "\n",
    "c = np.dot(a, b)\n",
    "\n",
    "print(f\"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \") \n",
    "c = np.dot(b, a)\n",
    "print(f\"NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.dot(a, b)= 2501072.5817\n",
      "Vectorized version duration : 15.62ms\n",
      "my dot function(a , b)= 2.501e+06\n",
      "loop version duration :4.608e+03 ms\n"
     ]
    }
   ],
   "source": [
    "# vector vs loop function \n",
    "\n",
    "# vector are speed more than for loop function \n",
    "\n",
    "np.random.seed(1)\n",
    "a = np.random.rand(10000000)\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "tic = time.time() # capture start time \n",
    "c = np.dot(a ,b )\n",
    "toc = time.time() # capture the end time \n",
    "print(f\"np.dot(a, b)= {c:.4f}\")\n",
    "print(f\"Vectorized version duration : {1000*(toc-tic):.4}ms\")\n",
    "\n",
    "\n",
    "tic =  time.time() # capture start time \n",
    "\n",
    "c = dot_function(a,b)\n",
    "toc = time.time()  # capture the end time \n",
    "\n",
    "\n",
    "print(f\"my dot function(a , b)= {c:.4}\")\n",
    "print(f\"loop version duration :{1000*(toc-tic):.4} ms\")\n",
    "\n",
    "del(a);del(b) # remove these big arrays from memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotFunction(a, b): \n",
    "    \"our manual dot function with for loop \"\n",
    "\n",
    "    dot_value = 0\n",
    "    m = len(a)\n",
    "\n",
    "    for i in range(m):\n",
    "        dot_value += a*[i]*b[i]\n",
    "    return dot_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
