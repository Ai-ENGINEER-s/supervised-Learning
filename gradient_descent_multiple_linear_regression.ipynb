{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT DESCENT FOR MULTIPLE LINEAR REGRESSION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Goals\n",
    "Extend our regression model routines to support multiple features\n",
    "Extend data structures to support multiple features\n",
    "Rewrite prediction, cost and gradient routines to support multiple features\n",
    "Utilize NumPy np.dot to vectorize their implementations for speed and simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy , math \n",
    "\n",
    "import numpy as np \n",
    "np.set_printoptions(precision=2) # reduced display precision on numpy arrays \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "<a name=\"toc_15456_1.3\"></a>\n",
    "## 1.3 Notation\n",
    "Here is a summary of some of the notation you will encounter, updated for multiple features.  \n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example matrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (3, 4),X type :<class 'numpy.ndarray'>\n",
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "y shape : (3,) y Type : <class 'numpy.ndarray'>\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "# data is stored in numpy array/matrix \n",
    "\n",
    "print(f\"X shape : {X_train.shape},X type :{type(X_train)}\")\n",
    "print(X_train)\n",
    "print(f\"y shape : {y_train.shape} y Type : {type(y_train)}\" )\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winit shape : (4,), b_init type <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083 \n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "print(f\"winit shape : {w_init.shape}, b_init type {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use for loop to make prediction in our multiple linear regression \n",
    "\n",
    "\n",
    "def predic_single_loop(x ,w , b): \n",
    "    \"prediction made with for loop function \"\n",
    "    m = x.shape[0]\n",
    "    prediction_value = 0\n",
    "\n",
    "    for i in range(m): \n",
    "        prediction_value += x[i]*w[i] \n",
    "    total_predicted_value = prediction_value + b\n",
    "    return total_predicted_value\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of x vect : [2104    5    1   45]\n",
      "f_wb shape () , prediction : 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data \n",
    "\n",
    "x_vec = X_train[0,:]\n",
    "\n",
    "# make a prediction for that line \n",
    "\n",
    "f_wb = predic_single_loop(x_vec, w_init,b_init)\n",
    "print(f\"value of x vect : {x_vec}\")\n",
    "print(f\"f_wb shape {f_wb.shape} , prediction : {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single prediction vector with numpy \n",
    "\n",
    "\n",
    "def predict(x , w ,b): \n",
    "\n",
    "    \"make prediction using dot function from numpy \"\n",
    "\n",
    "    p = np.dot(x , w) + b \n",
    "\n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_first_row shape (4,), x_vec value : [2104    5    1   45]\n",
      "prediction  : 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data \n",
    "b_init = 785.1811367994083 \n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "x_train_first_row = X_train[0,:]\n",
    "print(f\"x_train_first_row shape { x_train_first_row.shape}, x_vec value : {x_train_first_row}\")\n",
    "\n",
    "\n",
    "# make a prediction \n",
    "\n",
    "f_wb = predict(x_train_first_row, w_init, b_init)\n",
    "print(f\"prediction  : {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# 4 Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x,  y , w ,b ): \n",
    "\n",
    "    m = x.shape[0]\n",
    "    cost = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(x[i], w) +b \n",
    "        cost += (f_wb_i - y[i])**2\n",
    "    cost /= (2*m)\n",
    "\n",
    "    return cost \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904428966628e-12\n"
     ]
    }
   ],
   "source": [
    "# compute and display cost using our pre-chosen optimal parameters \n",
    "\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.1\"></a>\n",
    "## 5.1 Compute Gradient with Multiple Variables\n",
    "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x , y ,w ,b ): \n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "    for i in range(m):\n",
    "\n",
    "        err = (np.dot(x[i], w) +b ) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err*x[i , j]\n",
    "        dj_db = dj_db + err \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "\n",
    "    return  dj_db , dj_dw\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
