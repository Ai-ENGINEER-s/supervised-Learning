{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT DESCENT FOR MULTIPLE LINEAR REGRESSION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Goals\n",
    "Extend our regression model routines to support multiple features\n",
    "Extend data structures to support multiple features\n",
    "Rewrite prediction, cost and gradient routines to support multiple features\n",
    "Utilize NumPy np.dot to vectorize their implementations for speed and simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy , math \n",
    "\n",
    "import numpy as np \n",
    "np.set_printoptions(precision=2) # reduced display precision on numpy arrays \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "<a name=\"toc_15456_1.3\"></a>\n",
    "## 1.3 Notation\n",
    "Here is a summary of some of the notation you will encounter, updated for multiple features.  \n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example matrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (3, 4),X type :<class 'numpy.ndarray'>\n",
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "y shape : (3,) y Type : <class 'numpy.ndarray'>\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "# data is stored in numpy array/matrix \n",
    "\n",
    "print(f\"X shape : {X_train.shape},X type :{type(X_train)}\")\n",
    "print(X_train)\n",
    "print(f\"y shape : {y_train.shape} y Type : {type(y_train)}\" )\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winit shape : (4,), b_init type <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083 \n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "print(f\"winit shape : {w_init.shape}, b_init type {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use for loop to make prediction in our multiple linear regression \n",
    "\n",
    "\n",
    "def predic_single_loop(x ,w , b): \n",
    "    \"prediction made with for loop function \"\n",
    "    m = x.shape[0]\n",
    "    prediction_value = 0\n",
    "\n",
    "    for i in range(m): \n",
    "        prediction_value += x[i]*w[i] \n",
    "    total_predicted_value = prediction_value + b\n",
    "    return total_predicted_value\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of x vect : [2104    5    1   45]\n",
      "f_wb shape () , prediction : 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data \n",
    "\n",
    "x_vec = X_train[0,:]\n",
    "\n",
    "# make a prediction for that line \n",
    "\n",
    "f_wb = predic_single_loop(x_vec, w_init,b_init)\n",
    "print(f\"value of x vect : {x_vec}\")\n",
    "print(f\"f_wb shape {f_wb.shape} , prediction : {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single prediction vector with numpy \n",
    "\n",
    "\n",
    "def predict(x , w ,b): \n",
    "\n",
    "    \"make prediction using dot function from numpy \"\n",
    "\n",
    "    p = np.dot(x , w) + b \n",
    "\n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_first_row shape (4,), x_vec value : [2104    5    1   45]\n",
      "prediction  : 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data \n",
    "b_init = 785.1811367994083 \n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "x_train_first_row = X_train[0,:]\n",
    "print(f\"x_train_first_row shape { x_train_first_row.shape}, x_vec value : {x_train_first_row}\")\n",
    "\n",
    "\n",
    "# make a prediction \n",
    "\n",
    "f_wb = predict(x_train_first_row, w_init, b_init)\n",
    "print(f\"prediction  : {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# 4 Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x,  y , w ,b ): \n",
    "\n",
    "    m = x.shape[0]\n",
    "    cost = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(x[i], w) +b \n",
    "        cost += (f_wb_i - y[i])**2\n",
    "    cost /= (2*m)\n",
    "\n",
    "    return cost \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904428966628e-12\n"
     ]
    }
   ],
   "source": [
    "# compute and display cost using our pre-chosen optimal parameters \n",
    "\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5.1\"></a>\n",
    "## 5.1 Compute Gradient with Multiple Variables\n",
    "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x , y ,w ,b ): \n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "    for i in range(m):\n",
    "\n",
    "        err = (np.dot(x[i], w) +b ) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err*x[i , j]\n",
    "        dj_db = dj_db + err \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "\n",
    "    return  dj_db , dj_dw\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient pour le poids :  -5.0\n",
      "Gradient pour le biais :  -5.0\n"
     ]
    }
   ],
   "source": [
    "# implementation de la regression lineaire pour comprendre la difference entre les deux \n",
    "\n",
    "\n",
    "\n",
    "# N1 regression lineaire pour une seule variable \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([2,4,6])\n",
    "\n",
    "w = 0.5 \n",
    "b = 1.0\n",
    "\n",
    "# Fonction des calculs des gradients \n",
    "\n",
    "def compute_gradient_single_variable(x , y ,w , b):\n",
    "    m = len(x)\n",
    "    dj_dw =0.\n",
    "    dj_db = 0. \n",
    "\n",
    "    for i in range(m):\n",
    "        err = ((w*x[i] +b)- y[i])\n",
    "        dj_dw  += err * x[i]\n",
    "        dj_db += err\n",
    "\n",
    "    # Moyenne des gradients \n",
    "\n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "\n",
    "    return dj_dw , dj_dw \n",
    "\n",
    "\n",
    "# Exemple d'utilitsation \n",
    "\n",
    "\n",
    "dj_dw , dj_db = compute_gradient_single_variable(x , y , w , b)\n",
    "\n",
    "\n",
    "print(\"Gradient pour le poids : \",dj_dw)\n",
    "print(\"Gradient pour le biais : \", dj_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_mutiple_variable(x , y ,w ,b ): \n",
    "\n",
    "\n",
    "    m,n = x.shape \n",
    "\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "\n",
    "    for i in range(m):\n",
    "\n",
    "\n",
    "        err =( np.dot(x[i], w) + b ) - y[i]\n",
    "\n",
    "        for j in range(n): \n",
    "\n",
    "\n",
    "            dj_dw[j] += err * x[i, j]\n",
    "        dj_db += err \n",
    "        # mettre a jour le biais \n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "    dj_dw = dj_dw / m \n",
    "\n",
    "    dj_db = dj_db / m \n",
    "\n",
    "    return dj_dw , dj_db  \n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n",
      "dj_dw at initial w,b: \n",
      " -1.6739251501955248e-06\n"
     ]
    }
   ],
   "source": [
    "# compute and display gradient \n",
    "\n",
    "\n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient_mutiple_variable(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 GRADIENT DESCENT WITH MULTIPLE VARIABLES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      6\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5.0e-7\u001b[39m\n\u001b[1;32m----> 8\u001b[0m w_final, b_final, J_hist  \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcompute_cost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_gradient_mutiple_variable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[95], line 41\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Print cost every at intervals 10 times or as many iterations if < 10\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(num_iters \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Cost \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mJ_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m w, b, J_history\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "\n",
    "w_final, b_final, J_hist  = gradient_descent(X_train, y_train,initial_w, initial_b,compute_cost, compute_gradient_mutiple_variable, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost( x , y , w ,b): \n",
    "\n",
    "    m = x.shape[0]\n",
    "    f_wb = 0.0\n",
    "    for i in range(m) : \n",
    "        f_wb_i = (np.dot(x[i], w) +b)\n",
    "\n",
    "        f_wb += (f_wb_i - y[i])**2\n",
    "    \n",
    "    total_cost = f_wb / (2*m)\n",
    "\n",
    "    return total_cost \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x , y , w, b):\n",
    "    m, n = x.shape \n",
    "\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        err =( np.dot(x[i], w) +b ) - y[i]\n",
    "\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * x[i, j]\n",
    "        \n",
    "        dj_db += err\n",
    "\n",
    "    dj_db = dj_db / m \n",
    "    dj_dw = dj_dw / m \n",
    "\n",
    "    return dj_dw , dj_db\n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "               \n",
    "               \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n",
      "dj_dw at initial w,b: \n",
      " -1.6739251501955248e-06\n"
     ]
    }
   ],
   "source": [
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x , y , w_in ,b_in , cost_function , gradient_function , alpha , num_iters ) : \n",
    "    \"\"\"   Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)\"\"\"\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exemple sans la fonction deep_copy : [789, 8, 89] \n",
      " et ceci est la valeur initiale de a : [789, 8, 89]\n",
      "Utilisation avec deepcopy \n",
      "tab_1 values rest inchanged : [1, 78, 96] \n",
      " tab_3 value got changed : [2356, 78, 96]\n"
     ]
    }
   ],
   "source": [
    "# comprendre deep copy \n",
    "\n",
    "# c'est une fonction qui permet de garder les valeurs authentiques \n",
    "\n",
    "\n",
    "# example without deepcopy \n",
    "a = [1, 8 , 89]\n",
    "b = a \n",
    "\n",
    "b[0]= 789\n",
    "\n",
    "print(f\"exemple sans la fonction deep_copy : {b} \\n et ceci est la valeur initiale de a : {a}\")\n",
    "\n",
    "print(\"Utilisation avec deepcopy \")\n",
    "\n",
    "tab_1 = [1, 78, 96]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tab_3 = copy.deepcopy(tab_1)\n",
    "\n",
    "\n",
    "tab_3[0] = 2356\n",
    "\n",
    "print(f\"tab_1 values rest inchanged : {tab_1} \\n tab_3 value got changed : {tab_3}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
