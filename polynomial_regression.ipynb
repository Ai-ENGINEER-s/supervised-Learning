{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression implemnentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import math  , copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='FeatureEng'></a>\n",
    "# Feature Engineering and Polynomial Regression Overview\n",
    "\n",
    "Out of the box, linear regression provides a means of building models of the form:\n",
    "$$f_{\\mathbf{w},b} = w_0x_0 + w_1x_1+ ... + w_{n-1}x_{n-1} + b \\tag{1}$$ \n",
    "What if your features/data are non-linear or are combinations of features? For example,  Housing prices do not tend to be linear with living area but penalize very small or very large houses resulting in the curves shown in the graphic above. How can we use the machinery of linear regression to fit this curve? Recall, the 'machinery' we have is the ability to modify the parameters $\\mathbf{w}$, $\\mathbf{b}$ in (1) to 'fit' the equation to the training data. However, no amount of adjusting of $\\mathbf{w}$,$\\mathbf{b}$ in (1) will achieve a fit to a non-linear curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create run run_gradient_descent_feng\n",
    "\n",
    "\n",
    "def compute_cost(x , y , w, b):\n",
    "    m = x.shape[0]\n",
    "    cost = 0.0\n",
    "\n",
    "    for i in range(m): \n",
    "\n",
    "        f_wb_i = np.dot(x[i], w )\n",
    "\n",
    "        f_wb = (f_wb_i - y[i]) **2\n",
    "\n",
    "        cost+= f_wb\n",
    "    cost/=(2*m)\n",
    "\n",
    "    return cost \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient(x , y , w , b ):\n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(x[i], w) +b \n",
    "        f_wb = f_wb_i - y[i]\n",
    "\n",
    "        for j in range(n) :\n",
    "            dj_dw[j] += x[i, j] * f_wb\n",
    "        dj_db += f_wb \n",
    "\n",
    "    dj_dw /= m \n",
    "    dj_dw /= m \n",
    "\n",
    "    return  dj_dw , dj_db\n",
    "\n",
    "\n",
    "# now we have cost compute function => cost value \n",
    "\n",
    "# we also have the gradient compute function that return \n",
    "# the value of dj_dw , dj-db \n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(x , y ,w_in ,b_in , compute_cost, compute_gradient , alpha , number_iterations): \n",
    "\n",
    "\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b =b_in\n",
    "\n",
    "    for i in range(number_iterations):\n",
    "\n",
    "        dj_dw , dj_db = compute_gradient(x , y , w , b)\n",
    "\n",
    "        # mise a jours des gradients descents \n",
    "\n",
    "        w = w - alpha*dj_dw\n",
    "        b = b- alpha*dj_db\n",
    "\n",
    "        if i % 10 ==0:\n",
    "\n",
    "            J_history.append(compute_cost(x , y ,w , b))\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "    return w ,b ,J_history \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b,w found by gradient descent: -0.01,[ 0.2027783   0.00156243 -0.00365863 -0.01889488] \n",
      "prediction: 425.79, target value: 460\n",
      "prediction: 286.37, target value: 232\n",
      "prediction: 172.10, target value: 178\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618]) \n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "alpha = 5.0e-7\n",
    "iterations = 1000\n",
    "\n",
    "w_final , b_final , J_his = gradient_descent(X_train, y_train,initial_w , initial_b,compute_cost, compute_gradient, alpha ,iterations)\n",
    "\n",
    "m,_ = X_train.shape\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot numbers "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
