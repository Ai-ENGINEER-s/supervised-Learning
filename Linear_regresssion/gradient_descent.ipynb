{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT Descent for Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy : is popular library for scientific computaing \n",
    "# matplotlib : it's also a popular library for plotting data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math , copy \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 2.0])\n",
    "y_train = np.array([300.0 , 500.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x , y , w ,b ):\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w*x[i]+b \n",
    "        cost = cost + (f_wb - y)**2\n",
    "    cost_total_value = 1 /(2*m)*cost\n",
    "\n",
    "    return cost_total_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x , y ,w ,b ): \n",
    "    \n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db =0 \n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w*x[i]+ b \n",
    "        dj_dw_i = (f_wb - y[i])*x[i]\n",
    "        dj_db_i = (f_wb - y[i])\n",
    "    dj_dw = (dj_dw_i)/ m\n",
    "    dj_db = (dj_db_i) / m \n",
    "\n",
    "    return dj_db , dj_dw\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : Target values\n",
    "      w (scalar)        : Model parameter\n",
    "      b (scalar)        : Model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (float): The cost of using w, b as parameters for linear regression.\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    cost = (1 / (2 * m)) * np.sum((w * x + b - y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def compute_gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): Target values\n",
    "      w,b (scalar)    : Model parameters\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameter w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(x)\n",
    "    dj_dw = (1 / m) * np.sum((w * x + b - y) * x)\n",
    "    dj_db = (1 / m) * np.sum(w * x + b - y)\n",
    "    \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : Target values\n",
    "      w_in,b_in (scalar): Initial values of model parameters  \n",
    "      alpha (float)     : Learning rate\n",
    "      num_iters (int)   : Number of iterations to run gradient descent\n",
    "      cost_function     : Function to call to produce cost\n",
    "      gradient_function : Function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (List): History of parameters [w,b] \n",
    "    \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    \n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        # Update Parameters using the gradient descent rule\n",
    "        b = b - alpha * dj_db\n",
    "        w = w - alpha * dj_dw\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhau stion\n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            p_history.append([w, b])\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e}  \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "    \n",
    "    return w, b, J_history, p_history  # return w, b and cost, w history for graphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1.71e+01   dj_dw: -2.200e+01, dj_db: -6.000e+00   w:  2.200e-01, b: 6.00000e-02\n",
      "Iteration  100: Cost 1.71e-02   dj_dw: -2.042e-02, dj_db:  7.342e-02   w:  1.880e+00, b: 4.33677e-01\n",
      "Iteration  200: Cost 1.22e-02   dj_dw: -1.717e-02, dj_db:  6.201e-02   w:  1.899e+00, b: 3.66176e-01\n",
      "Iteration  300: Cost 8.70e-03   dj_dw: -1.450e-02, dj_db:  5.235e-02   w:  1.914e+00, b: 3.09179e-01\n",
      "Iteration  400: Cost 6.20e-03   dj_dw: -1.224e-02, dj_db:  4.421e-02   w:  1.928e+00, b: 2.61055e-01\n",
      "Iteration  500: Cost 4.42e-03   dj_dw: -1.034e-02, dj_db:  3.732e-02   w:  1.939e+00, b: 2.20421e-01\n",
      "Iteration  600: Cost 3.15e-03   dj_dw: -8.729e-03, dj_db:  3.152e-02   w:  1.948e+00, b: 1.86112e-01\n",
      "Iteration  700: Cost 2.25e-03   dj_dw: -7.370e-03, dj_db:  2.661e-02   w:  1.956e+00, b: 1.57143e-01\n",
      "Iteration  800: Cost 1.60e-03   dj_dw: -6.223e-03, dj_db:  2.247e-02   w:  1.963e+00, b: 1.32683e-01\n",
      "Iteration  900: Cost 1.14e-03   dj_dw: -5.255e-03, dj_db:  1.897e-02   w:  1.969e+00, b: 1.12031e-01\n",
      "Final values: w = 1.9737548787242036, b = 0.09475321533750963\n"
     ]
    }
   ],
   "source": [
    "# Initialiser les paramètres\n",
    "w_in = 0  # valeur initiale pour w\n",
    "b_in = 0  # valeur initiale pour b\n",
    "alpha = 0.01  # taux d'apprentissage\n",
    "num_iters = 1000  # nombre d'itérations\n",
    "\n",
    "# Exemple de données d'entraînement (x_train, y_train)\n",
    "x_train = np.array([1, 2, 3, 4, 5])  # Taille\n",
    "y_train = np.array([2, 4, 6, 8, 10])  # Prix\n",
    "\n",
    "# Exécuter la descente de gradient\n",
    "w, b, J_history, p_history = gradient_descent(x_train, y_train, w_in, b_in, alpha, num_iters, compute_cost, compute_gradient)\n",
    "\n",
    "print(f\"Final values: w = {w}, b = {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rebuild this thing again \n",
    "\n",
    "\n",
    "def compute_cost(x , y, w , b ):\n",
    "\n",
    "    m = x.shape[0]\n",
    "\n",
    "    f_wb = w*x + b\n",
    "    cost_function = np.sum((f_wb-y)**2)\n",
    "    total_costFunction = (1 / (2*m)*(cost_function))\n",
    "\n",
    "    return total_costFunction  \n",
    "\n",
    "\n",
    "def compute_gradient(x , y, w ,b ): \n",
    "\n",
    "    m = len(x)\n",
    "\n",
    "    dj_dw = 0 \n",
    "    dj_db = 0 \n",
    "\n",
    "    dj_dw =  np.sum(x * (w*x + b - y))\n",
    "    dj_db = np.sum((w*x +b - y))\n",
    "\n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    b = b_in\n",
    "    w= w_in\n",
    "\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        # Update Parameters using the gradient descent rule\n",
    "        b = b - alpha * dj_db\n",
    "        w = w - alpha * dj_dw\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhaustion\n",
    "            J_history.append(cost_function(x, y, w, b))\n",
    "            p_history.append([w, b])\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e}  \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "    \n",
    "    return w, b, J_history, p_history  # return w, b and cost, w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 1.71e+01   dj_dw: -2.200e+01, dj_db: -6.000e+00   w:  2.200e-01, b: 6.00000e-02\n",
      "Iteration  100: Cost 1.71e-02   dj_dw: -2.042e-02, dj_db:  7.342e-02   w:  1.880e+00, b: 4.33677e-01\n",
      "Iteration  200: Cost 1.22e-02   dj_dw: -1.717e-02, dj_db:  6.201e-02   w:  1.899e+00, b: 3.66176e-01\n",
      "Iteration  300: Cost 8.70e-03   dj_dw: -1.450e-02, dj_db:  5.235e-02   w:  1.914e+00, b: 3.09179e-01\n",
      "Iteration  400: Cost 6.20e-03   dj_dw: -1.224e-02, dj_db:  4.421e-02   w:  1.928e+00, b: 2.61055e-01\n",
      "Iteration  500: Cost 4.42e-03   dj_dw: -1.034e-02, dj_db:  3.732e-02   w:  1.939e+00, b: 2.20421e-01\n",
      "Iteration  600: Cost 3.15e-03   dj_dw: -8.729e-03, dj_db:  3.152e-02   w:  1.948e+00, b: 1.86112e-01\n",
      "Iteration  700: Cost 2.25e-03   dj_dw: -7.370e-03, dj_db:  2.661e-02   w:  1.956e+00, b: 1.57143e-01\n",
      "Iteration  800: Cost 1.60e-03   dj_dw: -6.223e-03, dj_db:  2.247e-02   w:  1.963e+00, b: 1.32683e-01\n",
      "Iteration  900: Cost 1.14e-03   dj_dw: -5.255e-03, dj_db:  1.897e-02   w:  1.969e+00, b: 1.12031e-01\n",
      "Final values: w = 1.9737548787242036, b = 0.09475321533750963\n"
     ]
    }
   ],
   "source": [
    "# Initialiser les paramètres\n",
    "w_in = 0  # valeur initiale pour w\n",
    "b_in = 0  # valeur initiale pour b\n",
    "alpha = 0.01  # taux d'apprentissage\n",
    "num_iters = 1000  # nombre d'itérations\n",
    "\n",
    "# Exemple de données d'entraînement (x_train, y_train)\n",
    "x_train = np.array([1, 2, 3, 4, 5])  # Taille\n",
    "y_train = np.array([2, 4, 6, 8, 10])  # Prix\n",
    "\n",
    "# Exécuter la descente de gradient\n",
    "w, b, J_history, p_history = gradient_descent(x_train, y_train, w_in, b_in, alpha, num_iters, compute_cost, compute_gradient)\n",
    "\n",
    "print(f\"Final values: w = {w}, b = {b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression for Multiple features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduction to numpy for multiple linear regression \n",
    "import numpy as np \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4) :   a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.zeros(4,) :  a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.random.random_sample(4): a = [0.43163741 0.27258306 0.07121235 0.04438798], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "# NumPy routines which allocate memory and fill arrays with value\n",
    "a = np.zeros(4);                print(f\"np.zeros(4) :   a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.zeros((4,));             print(f\"np.zeros(4,) :  a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.random_sample(4); print(f\"np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " values of [0. 1. 2. 3.]\n",
      "New values of variable a is [0.75420161 0.36224288 0.77241531 0.65839394]\n",
      "values of [0 1 2 3 4 5 6 7 8 9]\n",
      "the first element of a is 0\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(4.)\n",
    "print(f\" values of {a}\")\n",
    "a = np.random.rand(4)\n",
    "print(f\"New values of variable a is {a}\")\n",
    "\n",
    "# operations on vectors \n",
    "\n",
    "a = np.arange(10)\n",
    "print(f\"values of {a}\")\n",
    "\n",
    "# access an elment of a \n",
    "\n",
    "a_1 = a[0]\n",
    "print(f\"the first element of a is {a_1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_function(a , b): \n",
    "    \"compute dot product for vector with the first solution for loop \"\n",
    "\n",
    "    x = 0 \n",
    "    m = a.shape[0]\n",
    "\n",
    "    for i in range(m):\n",
    "        x = x + a[i]*b[i]\n",
    "    return x \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of our dot function : 74902\n"
     ]
    }
   ],
   "source": [
    "# test-1 \n",
    "a = np.array([41, 78 ,89])\n",
    "b = np.array([85, 45 ,763])\n",
    "\n",
    "print(f\"value of our dot function : {dot_function(a,b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 1-D np.dot(a, b) = 74902, np.dot(a, b).shape = () \n",
      "NumPy 1-D np.dot(b, a) = 74902, np.dot(a, b).shape = () \n"
     ]
    }
   ],
   "source": [
    "# Now let's try the second solution with numpy library dot function \n",
    "\n",
    "a = np.array([41, 78 ,89])\n",
    "b = np.array([85, 45 ,763])\n",
    "\n",
    "c = np.dot(a, b)\n",
    "\n",
    "print(f\"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \") \n",
    "c = np.dot(b, a)\n",
    "print(f\"NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.dot(a, b)= 2501072.5817\n",
      "Vectorized version duration : 19.85ms\n",
      "my dot function(a , b)= 2.501e+06\n",
      "loop version duration :5.296e+03 ms\n"
     ]
    }
   ],
   "source": [
    "# vector vs loop function \n",
    "\n",
    "# vector are speed more than for loop function \n",
    "\n",
    "np.random.seed(1)\n",
    "a = np.random.rand(10000000)\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "tic = time.time() # capture start time \n",
    "c = np.dot(a ,b )\n",
    "toc = time.time() # capture the end time \n",
    "print(f\"np.dot(a, b)= {c:.4f}\")\n",
    "print(f\"Vectorized version duration : {1000*(toc-tic):.4}ms\")\n",
    "\n",
    "\n",
    "tic =  time.time() # capture start time \n",
    "\n",
    "c = dot_function(a,b)\n",
    "toc = time.time()  # capture the end time \n",
    "\n",
    "\n",
    "print(f\"my dot function(a , b)= {c:.4}\")\n",
    "print(f\"loop version duration :{1000*(toc-tic):.4} ms\")\n",
    "\n",
    "del(a);del(b) # remove these big arrays from memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotFunction(a, b): \n",
    "    \"our manual dot function with for loop \"\n",
    "\n",
    "    dot_value = 0\n",
    "    m = len(a)\n",
    "\n",
    "    for i in range(m):\n",
    "        dot_value += a*[i]*b[i]\n",
    "    return dot_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full implementing of gradient descent in multiple linear regression \n",
    "\n",
    "\n",
    "def compute_cost(x , y ,w , b): \n",
    "    m = x.shape[0]\n",
    "    cost = 0.0\n",
    "\n",
    "    for i in range(m) :\n",
    "        f_wb_i = np.dot(x[i], w) + b\n",
    "        cost += (f_wb_i - y[i]) **2 \n",
    "    cost /= (2*m)\n",
    "\n",
    "    return cost \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at optimal w :1.5578904428966628e-12\n"
     ]
    }
   ],
   "source": [
    "# compute and display cost using our pre-chosen optimal parameters\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618]) \n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "cost = compute_cost(X_train  , y_train , w_init  , b_init)\n",
    "print(f'cost at optimal w :{cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x , y , w ,b):\n",
    "\n",
    "    m ,n = x.shape\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0.0\n",
    "\n",
    "    for i in range(m) : \n",
    "\n",
    "        f_wb_i = np.dot(x[i] , w) + b \n",
    "        f_wb = f_wb_i - y[i]\n",
    "\n",
    "        for j in range(n) :\n",
    "            dj_dw[j] += f_wb * x[i, j]\n",
    "        \n",
    "        dj_db += f_wb\n",
    "    dj_dw /= m\n",
    "\n",
    "    dj_db /= m \n",
    "\n",
    "\n",
    "    return dj_dw , dj_db\n",
    "             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: [-2.72623577e-03 -6.27197263e-06 -2.21745578e-06 -6.92403391e-05]\n",
      "dj_dw at initial w,b: \n",
      " -1.6739251501955248e-06\n"
     ]
    }
   ],
   "source": [
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent with Multiple variables \n",
    "\n",
    "\n",
    "def gradient_descent(x , y ,w_in ,b_in , compute_cost , compute_gradient , alpha , num_iters):\n",
    "\n",
    "    \"\"\" Perform batch gradient descent to learn w and b . Updates w and b \n",
    "    by taking num_iters steps with learning rate alpha \n",
    "\n",
    "    Args : \n",
    "    x : data , m exampes with n features \n",
    "    y : target values \n",
    "\n",
    "    w_in : initial model parameter \n",
    "    b_in(scalar) : initial model parameter \n",
    "    cost_function : function to compute the cost \n",
    "     gradient_function : function to cimpute the gradient \n",
    "\n",
    "     alpha : Learning rate \n",
    "\n",
    "     num_iters(int) : number of iterations to run gradient descent \n",
    "\n",
    "     Returns : \n",
    "\n",
    "     w  : Updated values of parameters \n",
    "     b : Updated value of parameter \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # we need an array to store J and w 's at each iteration primarily for graphing \n",
    "\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "\n",
    "\n",
    "    for i in range(num_iters) : \n",
    "        # calculate the gradient descent and update the parameter \n",
    "        dj_dw , dj_wb  = compute_gradient(x , y , w ,b)\n",
    "        # Update the parameter using  w , b alpha and gradient \n",
    "\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha *dj_wb\n",
    "\n",
    "        # save the cost at each iteration \n",
    "        if i < 1000000:\n",
    "            J_history.append(compute_cost(x , y ,w ,b))\n",
    "        \n",
    "        # print cost every at interval 10 times or as many iteration if the condition is verified \n",
    "\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d} : Cost {J_history[-1]:8.2f}\")\n",
    "    \n",
    "    return w , b , J_history # return final w , b  j history for graphing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 : Cost  2529.46\n",
      "Iteration  100 : Cost   695.99\n",
      "Iteration  200 : Cost   694.92\n",
      "Iteration  300 : Cost   693.86\n",
      "Iteration  400 : Cost   692.81\n",
      "Iteration  500 : Cost   691.77\n",
      "Iteration  600 : Cost   690.73\n",
      "Iteration  700 : Cost   689.71\n",
      "Iteration  800 : Cost   688.70\n",
      "Iteration  900 : Cost   687.69\n",
      "b,w found by gradient descent: -0.00,[ 0.20396569  0.00374919 -0.0112487  -0.0658614 ] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "alpha = 5.0e-7\n",
    "iterations = 1000\n",
    "\n",
    "w_final , b_final , J_his = gradient_descent(X_train, y_train,initial_w , initial_b,compute_cost, compute_gradient, alpha , num_iters)\n",
    "\n",
    "m,_ = X_train.shape\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
